{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "InDCMr5tnCrM",
        "1J3Lvghf2FS8",
        "mhT7tN-cysTF",
        "qlb7n_wYpMBf",
        "3P_JIcUTgOMW",
        "96Dvyi2tjizp",
        "sNJMIHxtokgq",
        "GK-D4ldQg55e",
        "PH_yoOo2XBFO",
        "eRTQUHlpqusP",
        "vYFsBi48f0L6",
        "XoSOBMZUf48_",
        "GEGP4UtSpL0F",
        "8wtXvSJKXHoG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRPhOfJQyxzC"
      },
      "source": [
        "# Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e44TPMq0nQSO"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InDCMr5tnCrM"
      },
      "source": [
        "# Imports and Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3Lvghf2FS8"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Xmx_9Dlzn2",
        "cellView": "form"
      },
      "source": [
        "#@markdown Dataset\n",
        "#@markdown ---\n",
        "IMG_DIMS =\t64#@param {type:\"integer\"}\n",
        "NUM_CHANS = 3 #@param {type:\"slider\", min:1, max:4, step:1}\n",
        "#number maximum of images used from the dataset (For small RAMs...)\n",
        "MAX_DATA_SIZE = 50000 #@param {type:\"slider\", min:4000, max:50000, step:1000}\n",
        "SPLIT_TRAIN_TEST = True #@param {type:\"boolean\"}\n",
        "TRAIN_RATIO = 90 #@param {type:\"slider\", min:60, max:90, step:5}\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown Training parameters\n",
        "#@markdown ---\n",
        "latent_dim = 1024 #@param {type:\"slider\", min:64, max:2048, step:64}\n",
        "#default: 50\n",
        "NUM_EPOCHS = 200 #@param {type:\"slider\", min:10, max:500, step:10}\n",
        "#default: 128\n",
        "BATCH_SIZE = 128 #@param {type:\"slider\", min:32, max:512, step:32}\n",
        "INIT_LR = 05e-4 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown Benchmarks\n",
        "#@markdown --\n",
        "#Image quality in the benchmark images\n",
        "BENCH_IMG_QUA =\t64 #@param {type:\"integer\"}\n",
        "IMG_DIR = \"/content/lsun_bedroom/data0/\" #@param {type:\"string\"}\n",
        "OUTPUT_PATH = \"/content/gdrive/MyDrive/Colab Files/Autoencoder/vae/\" #@param {type:\"string\"}\n",
        "SAMPLES_NUM = 8 #@param {type:\"slider\", min:8, max:30, step:1}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "#@markdown Resizing\n",
        "#@markdown ---\n",
        "SAVE_RESIZED = True #@param {type:\"boolean\"}\n",
        "RESIZED_DIR = \"/content/lsun_bedroom/\" #@param {type:\"string\"}\n",
        "RESIZED_PATH=RESIZED_DIR + \"trainImages_{}_{}.npy\".format(IMG_DIMS, MAX_DATA_SIZE)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhT7tN-cysTF"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dXixNIggroG"
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "K.clear_session()\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlb7n_wYpMBf"
      },
      "source": [
        "## Prepare Kaggle-cli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzzeSFfSnDnN"
      },
      "source": [
        "%%shell\n",
        "\n",
        "#!/bin/bash\n",
        "if [ -e \"~/.kaggle/kaggle.json\"]\n",
        "then\n",
        "\techo \"kaggle.json already exists\"\n",
        "\techo \"Skipping to next cell...\"\n",
        "else\n",
        "\techo \"Preparing Kaggle-cli...\"\n",
        "\tpip install -q kaggle\n",
        "\tpip install -q kaggle-cli\n",
        "\n",
        "\tmkdir -p ~/.kaggle\n",
        "\tcp \"/content/gdrive/MyDrive/Colab Files/Kaggle/kaggle.json\" ~/.kaggle/\n",
        "\tcat ~/.kaggle/kaggle.json \n",
        "\tchmod 600 ~/.kaggle/kaggle.json\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P_JIcUTgOMW"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96Dvyi2tjizp"
      },
      "source": [
        "## Download and Unzip The DataSet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-bFNwDWjy4u"
      },
      "source": [
        "%%shell\n",
        "\n",
        "#!/bin/bash\n",
        "if [ -e \"/content/lsun_bedroom.zip\" ]\n",
        "then\n",
        "\t\techo \"Dataset already downloaded\"\n",
        "\t\techo \"Skipping to next cell...\"\n",
        "else\n",
        "\t\techo \"Downloading Dataset...\"\n",
        "\t\tkaggle datasets download -d jhoward/lsun_bedroom\n",
        "\t\techo \"Unzipping Dataset...\"\n",
        "\t\tunzip \"/content/lsun_bedroom.zip\" -d \"/content/lsun_bedroom\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNJMIHxtokgq"
      },
      "source": [
        "## Prepare and Save The DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOqbQtPY0IF1"
      },
      "source": [
        "def prepare_dataset(ds_exists= False, img_dim = IMG_DIMS, save_resized = SAVE_RESIZED, resized_path = RESIZED_PATH,\n",
        "\t\t\t\t\t\t\t\t\t\tmax_data_size = MAX_DATA_SIZE, split_train_test = SPLIT_TRAIN_TEST, train_ratio = TRAIN_RATIO):\n",
        "\tif ds_exists:\n",
        "\t\ttotalImages = np.load(resized_path)\n",
        "\t\tif split_train_test:\n",
        "\t\t\t# take first train_len images (stop at the train_len'th image), the rest are the test images\n",
        "\t\t\ttrain_test=int(max_data_size*(train_ratio/100))\n",
        "\t\t\treturn totalImages[:train_test,:,:,:],totalImages[train_test:,:,:,:]\n",
        "\t\telse:\n",
        "\t\t\treturn totalImages, np.array()\n",
        "\telse:\n",
        "\t\timages=[]\n",
        "\t\tlength=1\n",
        "\t\tdim=0\n",
        "\t\tfor file_name in [os.path.join(dp, f) for dp, dn, fn in os.walk(IMG_DIR) for f in fn]:\n",
        "\t\t\timg = cv2.imread(file_name)\n",
        "\t\t\tresized = cv2.resize(img,(img_dim,img_dim))\n",
        "\t\t\timages.append(resized)\n",
        "\t\t\tlength += 1\n",
        "\t\t\tif length>max_data_size:\n",
        "\t\t\t\tbreak\n",
        "\t\ttotalImages=np.array(images)\n",
        "\t\tprint(totalImages)\n",
        "\t\tdel images\n",
        "\t\ttotalImages = totalImages.astype(\"float\")/255\n",
        "\t\tif save_resized:\n",
        "\t\t\tnp.save(resized_path,totalImages)\n",
        "\t\t\tprint(\"Resized images saved in path: \", RESIZED_PATH)\n",
        "\t\tif split_train_test:\n",
        "\t\t\ttrain_test=int(max_data_size*(train_ratio/100))\n",
        "\t\t\tprint (train_test)\n",
        "\t\t\treturn totalImages[:train_test,:,:,:],totalImages[train_test:,:,:,:]\n",
        "\t\telse:\n",
        "\t\t\treturn totalImages, np.array()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Baq6DrD8olNc"
      },
      "source": [
        "if os.path.isfile(RESIZED_PATH):\n",
        "\tprint(\".npy file already exists\")\n",
        "\tprint(\"Reading the existing file...\")\n",
        "\ttrainImages,testImages = prepare_dataset(ds_exists = True)\n",
        "else:\n",
        "\ttrainImages, testImages = prepare_dataset()\n",
        "\tprint(\"Datasaet pre\")\n",
        "print(trainImages.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK-D4ldQg55e"
      },
      "source": [
        "# VAE and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH_yoOo2XBFO"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRTQUHlpqusP"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCDgLjwnfHdS"
      },
      "source": [
        "def sampling(mu_log_variance):\n",
        "\tmu, log_variance = mu_log_variance\n",
        "\tepsilon = K.random_normal(K.shape(mu), mean=0.0, stddev=1.0)\n",
        "\trandom_sample = mu + K.exp(log_variance/2) * epsilon\n",
        "\treturn random_sample\n",
        "\n",
        "def loss_func(encoder_mu, encoder_log_variance):\n",
        "\tdef vae_reconstruction_loss(y_true, y_predict):\n",
        "\t\treconstruction_loss_factor = 1000\n",
        "\t\treconstruction_loss = K.mean(K.square(y_true-y_predict), axis=[1, 2, 3])\n",
        "\t\treturn reconstruction_loss_factor * reconstruction_loss\n",
        "\n",
        "\tdef vae_kl_loss(encoder_mu, encoder_log_variance):\n",
        "\t\tkl_loss = -0.5 * K.sum(1.0 + encoder_log_variance - K.square(encoder_mu) - K.exp(encoder_log_variance), axis=[1,2,3])\n",
        "\t\treturn kl_loss\n",
        "\n",
        "\tdef vae_kl_loss_metric(y_true, y_predict):\n",
        "\t\tkl_loss = -0.5 * K.sum(1.0 + encoder_log_variance - K.square(encoder_mu) - K.exp(encoder_log_variance), axis=1)\n",
        "\t\treturn kl_loss\n",
        "\n",
        "\tdef vae_loss(y_true, y_predict):\n",
        "\t\treconstruction_loss = vae_reconstruction_loss(y_true, y_predict)\n",
        "\t\tkl_loss = vae_kl_loss(y_true, y_predict)\n",
        "\n",
        "\t\tloss = reconstruction_loss + kl_loss\n",
        "\t\treturn loss\n",
        "\n",
        "\treturn vae_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYFsBi48f0L6"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vabKlpUg8BD"
      },
      "source": [
        "inputShape = (IMG_DIMS, IMG_DIMS, NUM_CHANS)\n",
        "\n",
        "input = Input(inputShape)\n",
        "\n",
        "enc = Conv2D(filters=NUM_CHANS, kernel_size=(3, 3), padding=\"same\", strides=1)(input)\n",
        "enc = BatchNormalization()(enc)\n",
        "enc = LeakyReLU()(enc)\n",
        "\n",
        "enc = Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", strides=1)(enc)\n",
        "enc = BatchNormalization()(enc)\n",
        "enc = LeakyReLU()(enc)\n",
        "\n",
        "enc = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", strides=2)(enc)\n",
        "enc = BatchNormalization()(enc)\n",
        "enc = LeakyReLU()(enc)\n",
        "\n",
        "enc = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", strides=2)(enc)\n",
        "enc = BatchNormalization()(enc)\n",
        "enc = LeakyReLU()(enc)\n",
        "\n",
        "enc = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", strides=2)(enc)\n",
        "enc = BatchNormalization()(enc)\n",
        "enc = LeakyReLU()(enc)\n",
        "\n",
        "enc = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", strides=2)(enc)\n",
        "enc = BatchNormalization()(enc)\n",
        "enc = LeakyReLU()(enc)\n",
        "\n",
        "enc = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", strides=1)(enc)\n",
        "enc = BatchNormalization()(enc)\n",
        "enc = LeakyReLU()(enc)\n",
        "\n",
        "shape_before_flatten = K.int_shape(enc)\n",
        "enc_flatten = Flatten()(enc)\n",
        " \n",
        "enc_mu = Dense(units=latent_dim)(enc_flatten)\n",
        "enc_log_variance = Dense(units=latent_dim)(enc_flatten)\n",
        "\n",
        "\n",
        "\n",
        "enc_output = Lambda(sampling)([enc_mu, enc_log_variance])\n",
        "\n",
        "# build the encoder model\n",
        "encoder = Model(input, enc_output)\n",
        "print(encoder.summary())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoSOBMZUf48_"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVPEpgqifyie"
      },
      "source": [
        "dec_input = Input(shape=(latent_dim,))\n",
        "dec = Dense(np.prod(shape_before_flatten[1:]))(dec_input)\n",
        "dec = Reshape(target_shape=shape_before_flatten[1:])(dec)\n",
        "\n",
        "\n",
        "dec = Conv2DTranspose(filters=128, kernel_size=(3, 3), padding=\"same\", strides=1)(dec)\n",
        "dec = BatchNormalization()(dec)\n",
        "dec = LeakyReLU()(dec)\n",
        "\n",
        "dec = Conv2DTranspose(filters=128, kernel_size=(3, 3), padding=\"same\", strides=2)(dec)\n",
        "dec = BatchNormalization()(dec)\n",
        "dec = LeakyReLU()(dec)\n",
        "\n",
        "dec = Conv2DTranspose(filters=128, kernel_size=(3, 3), padding=\"same\", strides=2)(dec)\n",
        "dec = BatchNormalization()(dec)\n",
        "dec = LeakyReLU()(dec)\n",
        "\n",
        "dec = Conv2DTranspose(filters=64, kernel_size=(3, 3), padding=\"same\", strides=2)(dec)\n",
        "dec = BatchNormalization()(dec)\n",
        "dec = LeakyReLU()(dec)\n",
        "\n",
        "dec = Conv2DTranspose(filters=64, kernel_size=(3, 3), padding=\"same\", strides=2)(dec)\n",
        "dec = BatchNormalization()(dec)\n",
        "dec = LeakyReLU()(dec)\n",
        "\n",
        "\n",
        "dec = Conv2DTranspose(filters=32, kernel_size=(3, 3), padding=\"same\", strides=1)(dec)\n",
        "dec = BatchNormalization()(dec)\n",
        "dec = LeakyReLU()(dec)\n",
        "\n",
        "dec = Conv2DTranspose(filters=NUM_CHANS, kernel_size=(3, 3), padding=\"same\", strides=1)(dec)\n",
        "dec = LeakyReLU()(dec)\n",
        "\n",
        "\n",
        "# build the decoder model\n",
        "decoder = Model(dec_input, dec)\n",
        "\n",
        "print(decoder.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEGP4UtSpL0F"
      },
      "source": [
        "### VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dOFLWJypJzA"
      },
      "source": [
        "vae = Model(input, decoder(encoder(input)))\n",
        "vae.summary()\n",
        "\n",
        "vae.compile(optimizer=Adam(lr=INIT_LR), loss=loss_func(enc_mu, enc_log_variance))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wtXvSJKXHoG"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm8C4zSAq_75"
      },
      "source": [
        "H = vae.fit(trainImages, trainImages, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, shuffle=True, validation_data=(testImages, testImages))\n",
        "vae.save(OUTPUT_PATH + \"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wdt3FGQkxVB"
      },
      "source": [
        "# construct a plot that plots and saves the training history\n",
        "N = np.arange(0, NUM_EPOCHS)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(OUTPUT_PATH + \"graph-4.png\")\n",
        "\n",
        "# use the convolutional autoencoder to make predictions on the\n",
        "# testing images, then initialize our list of output images\n",
        "print(\"[INFO] making predictions...\")\n",
        "decoded = vae.predict(testImages[:SAMPLES_NUM])\n",
        "outputs = None\n",
        "\n",
        "# loop over our number of output samples\n",
        "for i in range(0, SAMPLES_NUM):\n",
        "\t# grab the original image and reconstructed image\n",
        "\toriginal = (testImages[i] * 255).astype(\"uint8\")\n",
        "\trecon = (decoded[i] * 255).astype(\"uint8\")\n",
        "\n",
        "\t# stack the original and reconstructed image side-by-side\n",
        "\toutput = np.hstack([original, recon])\n",
        "\n",
        "\t# if the outputs array is empty, initialize it as the current side-by-side image display\n",
        "\tif outputs is None:\n",
        "\t\toutputs = output\n",
        "\t# otherwise, vertically stack the outputs\n",
        "\telse:\n",
        "\t\toutputs = np.vstack([outputs, output])\n",
        "\n",
        "# save the outputs image\n",
        "cv2.imwrite(OUTPUT_PATH + \"output_samples-4.png\", outputs)\n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}